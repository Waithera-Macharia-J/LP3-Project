{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Title: \n",
    "Predictive Modeling for Oil Sales and Store Performance(Time Series Forecasting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "### 1. Introduction\n",
    "This project focuses on time series forecasting to predict store sales for Corporation Favorita, a large Ecuadorian-based grocery retailer. The objective is to build a model that accurately predicts the unit sales for thousands of items sold at different Favorita stores.\n",
    "\n",
    "### 1.1. Objectives\n",
    "The goal of this project is to predict sales and optimize business strategies based on data provided. The data includes information on oil prices, holidays_events, store details, and sales transactions. The goal is to explore, preprocess, and develop regression models such as linear regression, XGBoost or ARIMA to predict and understand the factors influencing oil sales and store performance.\n",
    "\n",
    "### 1.2. Methodology\n",
    "To achieve the objectives, we will follow a structured approach:\n",
    "\n",
    "Data Exploration: Thoroughly explore the provided datasets to understand the available features, their distributions, and relationships. This step will provide initial insights into the store sales data and help identify any data quality issues.\n",
    "\n",
    "Data Preparation: Handle missing values, perform feature engineering, and encode categorical variables as necessary. This step may involve techniques like imputation, scaling, and one-hot encoding.\n",
    "\n",
    "Time Series Analysis: Analyze the temporal aspects of the data, including trends, seasonality, and potential outliers. This analysis will provide a deeper understanding of the underlying patterns in store sales over time.\n",
    "\n",
    "Model Selection and Training: Select appropriate time series forecasting models and train them using the prepared data. Consider incorporating external factors like promotions, holidays, and oil prices, if available, to enhance the forecasting accuracy.\n",
    "\n",
    "Model Evaluation: Evaluate the trained models using appropriate metrics, such as mean absolute error (MAE), root mean squared error (RMSE), or mean absolute percentage error (MAPE). Assess the models' performance and identify the most accurate and reliable forecasting model.\n",
    "\n",
    "Model Deployment and Forecasting: Deploy the chosen model to predict store sales for future time periods, leveraging the provided test dataset. Generate forecasts for the target period and assess the model's ability to capture the sales patterns accurately.\n",
    "\n",
    "By following this methodology, we aim to provide valuable insights to the telecom company and develop a reliable predictive model for customer churn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Hypothesis \n",
    "\n",
    "### Null Hypothesis (H0): \"There is no significant relationship between store sales and promotions.\"\n",
    "\n",
    "### Alternative Hypothesis (Ha): \"There is a significant relationship between store sales and churn promotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "1. s the train dataset complete (has all the required dates)?\n",
    "\n",
    "2. Which dates have the lowest and highest sales for each year?\n",
    "\n",
    "3. Did the earthquake impact sales?\n",
    "\n",
    "4. Are certain groups of stores selling more products? (Cluster, city, state, type)\n",
    "\n",
    "5. Are sales affected by promotions, oil prices and holidays?\n",
    "\n",
    "6. What analysis can we get from the date and its extractable features?\n",
    "\n",
    "7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Data Understanding\n",
    "\n",
    "Installing libraries and Packages\n",
    "\n",
    "\n",
    "In this section we will import all the packages/libraries that we will be using through this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Handling\n",
    "import pandas as pd  # For data manipulation and analysis using DataFrames\n",
    "import numpy as np  # For numerical operations and arrays\n",
    "import pyodbc  # For handling connections to Microsoft SQL Server\n",
    "from dotenv import dotenv_values  # For loading environment variables, possibly including database credentials\n",
    "import pingouin as pg\n",
    "\n",
    "# Statistical Analysis\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "import scipy.stats as stats\n",
    "import math  # Basic mathematical operations\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from scipy.stats import ttest_ind, t\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt  # For creating visualizations using Matplotlib\n",
    "import seaborn as sns  # Enhanced data visualization based on Matplotlib\n",
    "import plotly.express as px\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "# Feature Processing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#Machine Learning\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import pickle\n",
    "\n",
    "# # Other Packages\n",
    "import warnings  # To handle warnings in a way that they can be ignored\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"UID\")\n",
    "password = environment_variables.get(\"PWD\")\n",
    "\n",
    "# Connection string\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing first dataset from the database\n",
    "We want to access 3 files from a remote microsoft sql server. These are the files:\n",
    "1.\tdbo.oil\n",
    "2.\tdbo.holidays_events\n",
    "3.\tdbo.stores\n",
    "\n",
    "We will use the following packages:\n",
    "\n",
    "•\tpyodbc: A package for creating connection strings to the remote database\n",
    "\n",
    "•\tpython-dotenv: A package for creating environment variables that will help us hide sensitve configuration informantion such as database credentials and API keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define SQL queries to retrieve data from each table\n",
    "query_oil = \"SELECT * FROM dbo.oil\"\n",
    "query_holidays_events = \"SELECT * FROM dbo.holidays_events\"\n",
    "query_stores = \"SELECT * FROM dbo.stores\"\n",
    "\n",
    "# Retrieve data from each table\n",
    "oil_data = pd.read_sql(query_oil, connection)\n",
    "holidays_events_data = pd.read_sql(query_holidays_events, connection)\n",
    "stores_data = pd.read_sql(query_stores, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing second  dataset from the oneDdrive\n",
    "We want to access 2 files from a oneDrive CSV files. These are the files:\n",
    "1.\tsample_submission.csv\n",
    "2.\ttest.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file for submission data into DataFrames\n",
    "sample_submission_data = pd.read_csv('sample_submission.csv')\n",
    "sample_submission_data .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files for test data into DataFrames\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing third  dataset from the GitHub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file for transactions data into DataFrames\n",
    "transactions_data = pd.read_csv('transactions.csv')\n",
    "transactions_data .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data for oil\n",
    "oil_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape for oil dataframe\n",
    "oil_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the missing values using isnull\n",
    "oil_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing values using mean imputation\n",
    "oil_data['dcoilwtico'] = oil_data['dcoilwtico'].fillna(oil_data['dcoilwtico'].mean())\n",
    "# Check again for missing values\n",
    "oil_data['dcoilwtico'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for numerical columns\n",
    "oil_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format\n",
    "oil_data['date'] = pd.to_datetime(oil_data['date'])\n",
    "\n",
    "# Extract year from 'date' column\n",
    "oil_data['year'] = oil_data['date'].dt.year\n",
    "\n",
    "# Group data by year and calculate mean oil price for each year\n",
    "yearly_oil_prices = oil_data.groupby('year')['dcoilwtico'].mean().reset_index()\n",
    "\n",
    "# Visualize the distribution of oil prices\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(oil_data['date'], oil_data['dcoilwtico'])\n",
    "plt.title('Oil Sales Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Oil Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of oil prices over the years\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(yearly_oil_prices['year'], yearly_oil_prices['dcoilwtico'], marker='o')\n",
    "plt.title('Average Oil Prices Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Oil Price')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View data for Holiday\n",
    "holidays_events_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the shape for the holiday dataset\n",
    "holidays_events_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the missing values using isnull\n",
    "holidays_events_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in the 'type' column\n",
    "print(holidays_events_data['type'].value_counts())\n",
    "\n",
    "# Check unique values in the 'locale' column\n",
    "print(holidays_events_data['locale'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for numerical columns\n",
    "holidays_events_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#view dataset for stores\n",
    "stores_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape for store data\n",
    "stores_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the missing values using isnull\n",
    "stores_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for numerical columns\n",
    "stores_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataset for sample_submission\n",
    "sample_submission_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape for the sample_submission\n",
    "sample_submission_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dataset for test\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cheking the shape for test dataframe\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the missing values using isnull\n",
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for numerical columns\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view train data \n",
    "train_data .head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for the shape \n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the missing values using isnull\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for numerical columns\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming the 'date' column to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the 'date' column in the datasets to datetime format\n",
    "# Train dataset\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Test dataset\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "\n",
    "# Holiday Events dataset\n",
    "holidays_events_data['date'] = pd.to_datetime(holidays_events_data['date'])\n",
    "\n",
    "# Oil dataset\n",
    "oil_data['date'] = pd.to_datetime(oil_data['date'])\n",
    "\n",
    "# Transactions dataset\n",
    "transactions_data['date'] = pd.to_datetime(transactions_data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the data type of the 'date' column after transformation\n",
    "print('Date Column Data Type After Transformation:') \n",
    "print('==='*14)\n",
    "print(\"Train dataset:\", train_data['date'].dtype)\n",
    "print(\"Test dataset:\", test_data['date'].dtype)\n",
    "print(\"Holiday Events dataset:\", holidays_events_data['date'].dtype)\n",
    "print(\"Oil dataset:\", oil_data['date'].dtype)\n",
    "print(\"Transactions dataset:\", transactions_data['date'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Checking for the completeness of the 'date' column in the Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the completeness of the train dataset\n",
    "min_date = train_data['date'].min()\n",
    "max_date = train_data['date'].max()\n",
    "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "missing_dates = expected_dates[~expected_dates.isin(train_data['date'])]\n",
    "\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
    "else:\n",
    "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the missing dates in the train dataset\n",
    "# Create an index of the missing dates as a DatetimeIndex object\n",
    "missing_dates = pd.Index(['2013-12-25', '2014-12-25', '2015-12-25', '2016-12-25'], dtype='datetime64[ns]')\n",
    "\n",
    "# Create a DataFrame with the missing dates, using the 'date' column\n",
    "missing_data = pd.DataFrame({'date': missing_dates})\n",
    "\n",
    "# Concatenate the original train dataset and the missing data DataFrame\n",
    "# ignore_index=True ensures a new index is assigned to the resulting DataFrame\n",
    "train_data = pd.concat([train_data, missing_data], ignore_index=True)\n",
    "\n",
    "# Sort the DataFrame based on the 'date' column in ascending order\n",
    "train_data.sort_values('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the completeness of the train dataset\n",
    "min_date = train_data['date'].min()\n",
    "max_date = train_data['date'].max()\n",
    "expected_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "missing_dates = expected_dates[~expected_dates.isin(train_data['date'])]\n",
    "\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"The train dataset is complete. It includes all the required dates.\")\n",
    "else:\n",
    "    print(\"The train dataset is incomplete. The following dates are missing:\")\n",
    "    print(missing_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veiwing the transactions dataset\n",
    "transactions_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the shape\n",
    "transactions_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for the missing values using isnull\n",
    "transactions_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics for numerical columns\n",
    "transactions_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format\n",
    "transactions_data['date'] = pd.to_datetime(transactions_data['date'])\n",
    "\n",
    "# Group data by date and calculate total transactions for each date\n",
    "daily_total_transactions = transactions_data.groupby('date')['transactions'].sum().reset_index()\n",
    "\n",
    "# Visualize the distribution of total transactions over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(daily_total_transactions['date'], daily_total_transactions['transactions'], marker='o')\n",
    "plt.title('Total Transactions Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Total Transactions')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime format\n",
    "train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "\n",
    "# Create 'year' column\n",
    "train_data['year'] = train_data['date'].dt.year\n",
    "\n",
    "# Group data by year and find the index of the row with maximum sales for each year\n",
    "max_sales = train_data.groupby('year')['sales'].idxmax()\n",
    "\n",
    "# Select rows with maximum sales for each year\n",
    "Result_max = train_data.loc[max_sales]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "Result_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart of dates with highest sales each year\n",
    "\n",
    "sns.barplot(data=Result_max, y=\"date\", x=\"sales\", palette='crest')\n",
    "plt.ylabel(\"Date\")\n",
    "plt.xlabel(\"Total sales\")\n",
    "plt.title(\"Highest sales date in each year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Is the train dataset complete (has all the required dates)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if train_data.isnull().values.any():\n",
    "  print(\"The dataset is not complete. There are missing values.\")\n",
    "else:\n",
    "  print(\"The dataset is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing dates\n",
    "\n",
    "missing_dates = expected_dates.difference(train_data['date'].unique())\n",
    "missing_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Which dates have the lowest and highest sales for each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highest sales date in each year\n",
    "\n",
    "train_data['year'] = train_data['date'].dt.year\n",
    "max_sales = train_data.groupby('year')['sales'].idxmax()\n",
    "Result_max = train_data.loc[max_sales]\n",
    "Result_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart of dates with highest sales each year\n",
    "\n",
    "sns.barplot(data=Result_max, y=\"date\", x=\"sales\", palette='crest')\n",
    "plt.ylabel(\"Date\")\n",
    "plt.xlabel(\"Total sales\")\n",
    "plt.title(\"Highest sales date in each year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2016(11/12) has the highest number of sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowest sales date in each year\n",
    "\n",
    "min_sales = train_data.groupby('year')['sales'].idxmin()\n",
    "Result_min = train_data.loc[min_sales]\n",
    "Result_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horizontal bar chart of dates with lowest sales each year\n",
    "\n",
    "sns.barplot(data=Result_min, y=\"date\", x=\"sales\", palette='crest')\n",
    "plt.ylabel(\"Date\")\n",
    "plt.xlabel(\"Total sales\")\n",
    "plt.title(\"lowest sales date in each year\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Did the earthquake impact sales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the 'date' column as the index\n",
    "\n",
    "train_data.set_index('date', inplace=True)\n",
    "\n",
    "# Resample to weekly frequency, aggregating with mean\n",
    "\n",
    "sales_daily_mean = train_data[\"sales\"].resample('D').mean()\n",
    "sales_weekly_mean = train_data[\"sales\"].resample('W').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales for April to May 2016.\n",
    "start, end = '2016-04', '2016-05'\n",
    "\n",
    "# Analyse before and after the earthquake\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "\n",
    "color_daily = 'blue'\n",
    "color_weekly = 'green'\n",
    "\n",
    "ax.plot(sales_daily_mean.loc[start:end], marker='.', linestyle='-', linewidth=0.5, label='Daily', color=color_daily)\n",
    "ax.plot(sales_weekly_mean.loc[start:end], marker='o', markersize=8, linestyle='-', label='Weekly Mean Resample', color=color_weekly)\n",
    "ax.set_ylabel(\"Total sales\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.tick_params(axis='x', labelrotation=45)\n",
    "ax.legend()\n",
    "ax.set_title(\"April and May 2016 total sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Are certain groups of stores selling more products? (Cluster, city, state, type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and stores datasets\n",
    "\n",
    "train_stores = pd.merge(train_data, stores_data)\n",
    "\n",
    "train_stores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store sales by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of sales by cluster\n",
    "\n",
    "sales_clusters = train_stores.groupby(\"cluster\", as_index=False)[\"sales\"].sum()\n",
    "sales_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical Bar chart of sales by cluster\n",
    "\n",
    "ax = sns.barplot(data=sales_clusters, x = \"cluster\", y = \"sales\", palette='viridis')\n",
    "plt.xlabel(\"Stores by cluster\")\n",
    "plt.ylabel(\"Total sales\")\n",
    "plt.title(\"Sales of different Stores by cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cluster 14 has the highest sales. cluster 16 has the lowest sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store sales by city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of sales by city\n",
    "\n",
    "sales_city = train_stores.groupby(\"city\", as_index=False)[\"sales\"].sum()\n",
    "sales_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal Bar chart of sales by city\n",
    "\n",
    "sns.barplot(data=sales_city, y = \"city\", x = \"sales\", palette='viridis')\n",
    "plt.ylabel(\"City of  of stores\")\n",
    "plt.xlabel(\"Total sales\")\n",
    "plt.title(\"Sales of different Stores by City\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Quito has the highest total sales, as it is the capital, and Puyo the lowest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store sales by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of sales by state\n",
    "\n",
    "sales_state = train_stores.groupby(\"state\", as_index=False)[\"sales\"].sum()\n",
    "sales_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# horizontal Bar chart of sales by state\n",
    "\n",
    "sns.barplot(data=sales_state, y = \"state\", x = \"sales\", palette='viridis')\n",
    "plt.ylabel(\"States of stores\")\n",
    "plt.xlabel(\"Total sales\")\n",
    "plt.title(\"Sales of different Stores by State\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store sales by type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of sales by store type\n",
    "\n",
    "sales_type = train_stores.groupby(\"type\", as_index=False)[\"sales\"].sum()\n",
    "sales_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertical Bar chart of sales by store type\n",
    "\n",
    "sns.barplot(data=sales_type, x = \"type\", y = \"sales\", palette='viridis')\n",
    "plt.xlabel(\"Stores by store type\")\n",
    "plt.ylabel(\"Total sales\")\n",
    "plt.title(\"Sales of Stores by type\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Are sales affected by promotions, oil prices and holidays?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values product family\n",
    "\n",
    "train_data['family'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows with promotions and sum sales for top 10 families\n",
    "promotions = train_data[train_data[\"onpromotion\"] != 0].groupby(\"family\")[\"sales\"].sum().sort_values(ascending=False).head(10)\n",
    "\n",
    "# Filter rows without promotions and sum sales for top 10 families\n",
    "no_promotions = train_data[train_data[\"onpromotion\"] == 0].groupby(\"family\")[\"sales\"].sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stacked bar chart to visualize sales on and off promotions\n",
    "\n",
    "ax = promotions.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "no_promotions.plot(kind='bar', stacked=True, ax=ax, colormap='cividis')\n",
    "\n",
    "# Rotate x-axis labels\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add legend\n",
    "plt.legend([\"Promotions\", \"No Promotions\"])\n",
    "\n",
    "# Label the axes and title\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.xlabel(\"Products\")\n",
    "plt.title(\"Sales of products with or without promotion (Top 10)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales and oil prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train and oil dataset\n",
    "\n",
    "merge = train_data.merge(\n",
    "    oil_data, \n",
    "    how='left', \n",
    "    on=['date'])\n",
    "    \n",
    "merge.reset_index()\n",
    "merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# Plot the average sales by year \n",
    "merge.groupby(['year_y'])['sales'].sum().plot.bar(ax=ax1, title='Total Sales by Year')\n",
    "ax1.set_xlabel('year_y')\n",
    "ax1.set_ylabel('Total Sales')\n",
    "\n",
    "# Plot the oil data overtime\n",
    "sns.lineplot(data=merge, x='date', y='dcoilwtico', ax=ax2)\n",
    "ax2.set_title('Oil price overtime')\n",
    "ax2.set_xlabel('year_y')\n",
    "ax2.set_ylabel('oil price')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the correlation between sales and oil prices\n",
    "\n",
    "correlation = merge['sales'].corr(merge['dcoilwtico'])\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales and holidays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train and holidays dataset\n",
    "\n",
    "merge_2 = train_data.merge(\n",
    "    holidays_events_data, \n",
    "    how='left', \n",
    "    on=['date'])\n",
    "    \n",
    "merge_2.reset_index()\n",
    "merge_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by holiday type and calculate the average sales for each type\n",
    "avg_sales_by_type = merge_2.groupby(\"type\").agg({\"sales\": \"mean\"})\n",
    "\n",
    "# Create a figure and axis object\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot a bar chart of the average sales by holiday type\n",
    "avg_sales_by_type.plot(kind=\"bar\", y=\"sales\", ax=ax, legend=False)\n",
    "\n",
    "# Add labels to the bars\n",
    "for index, value in enumerate(avg_sales_by_type[\"sales\"]):\n",
    "    plt.text(index, value, str(round(value, 2)), ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "# Add a title and labels\n",
    "ax.set_title(\"Average Sales by Holiday Type\", fontsize=16)\n",
    "ax.set_xlabel(\"Holiday Type\", fontsize=14)\n",
    "ax.set_ylabel(\"Average Sales\", fontsize=14)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. What analysis can we get from the date and its extractable features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from date column\n",
    "\n",
    "def getDateFeatures(df, date):\n",
    "    df['date'] = pd.to_datetime(df[date])\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['Month'] = df['date'].dt.month_name()\n",
    "    df['Day'] = df['date'].dt.day_name()\n",
    "    df['day_of_month'] = df['date'].dt.day\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df[\"is_weekend\"] = np.where(df['day_of_week'] > 4, 1, 0)\n",
    "    df['quarter'] = df['date'].dt.quarter\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our train data through our function\n",
    "\n",
    "train_copy = train_data.reset_index()\n",
    "train_data = getDateFeatures(train_copy, \"date\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales on Pay Day (Semi-Monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list with semi monthly pay day range (15th and last day 30/31st of the month)\n",
    "\n",
    "pay_day = pd.date_range(start=train_data.date.min(), end=train_data.date.max(), freq='SM').astype('str').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create payday column\n",
    "\n",
    "train_data[\"pay_day\"] = np.where(train_data[\"date\"].isin(pay_day), 1, 0)\n",
    "train_data['pay_day'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average sales in relation to payday\n",
    "\n",
    "ax = train_data.groupby([\"pay_day\"], as_index=False)['sales'].mean()\\\n",
    "                                    .plot(\"pay_day\", \"sales\", kind=\"bar\", figsize=(12,6),\n",
    "                                         title=\"Average sales in relation to payday\")\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(format(p.get_height(), '.2f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 9), \n",
    "                textcoords = 'offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales by day of the month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of sales by day of the month\n",
    "# which day of the month has the most sales?\n",
    "\n",
    "ax = train_data.groupby([\"day_of_month\"], as_index=False)[\"sales\"].sum()\\\n",
    "                                    .plot(\"day_of_month\", \"sales\", kind=\"bar\", figsize=(12,6),\n",
    "                                         title=\"Total sales by day of month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales by week of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of sales by the weeks of the year\n",
    "\n",
    "sales_by_week = train_data.groupby([\"week_of_year\"], as_index=False)['sales'].sum()\n",
    "\n",
    "ax = sales_by_week.plot(\"week_of_year\", \"sales\", kind=\"bar\", figsize=(12, 6),\n",
    "                         title=\"Total sales over the weeks of the year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sales by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the sum of sales by month of the year\n",
    "# which month has the highest sales?\n",
    "\n",
    "purchase_month = train_data.groupby(\"Month\", as_index=False)[\"sales\"].sum().sort_values(by=\"sales\", ascending=False)\n",
    "purchase_month = purchase_month.reset_index(drop=True)\n",
    "purchase_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of sales by month of the year\n",
    "\n",
    "ax = purchase_month.plot(\"Month\", \"sales\", kind=\"bar\", figsize=(12, 6),\n",
    "                         title=\"Total sales over the months of the year\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales by Day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the sum of sales by day of the week\n",
    "# which day has the highest sales?\n",
    "\n",
    "purchase_day = train_data.groupby(\"Day\", as_index=False)[\"sales\"].sum().sort_values(by=\"sales\", ascending=False)\n",
    "purchase_day = purchase_day.reset_index(drop=True)\n",
    "purchase_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of sales by day of the week\n",
    "\n",
    "ax = purchase_day.plot(\"Day\", \"sales\", kind=\"bar\", figsize=(12, 6),\n",
    "                         title=\"Total sales over the days of the week\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales by season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for rainy and dry season\n",
    "\n",
    "train_data[\"season\"] = np.where(train_data[\"month\"].isin([6,7,8,9]), 1, 0)\n",
    "\n",
    "# plot sum of sales by season of the year (Dry or Rainy)\n",
    "sales_by_season = train_data.groupby([\"season\"], as_index=False)['sales'].sum()\n",
    "\n",
    "ax = sales_by_season.plot(\"season\", \"sales\", kind=\"bar\", figsize=(12, 6),\n",
    "                          title=\"Total sales over seasons in Ecuador (Dry=0, Rainy=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales by quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the sum of sales by quarter of the year\n",
    "\n",
    "sales_by_quarter = train_data.groupby([\"quarter\"], as_index=False)['sales'].sum()\n",
    "\n",
    "ax = sales_by_quarter.plot(\"quarter\", \"sales\", kind=\"bar\", figsize=(12, 6),\n",
    "                          title=\"Total sales by quarter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What is the difference between RMSLE, RMSE, MSE (or why is the MAE greater than all of them?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i. Root Mean Squared Logarithmic Error (RMSLE)\n",
    "- RMSLE is a logarithmic version of RMSE. It measures the ratio between the true and predicted values, rather than the difference.\n",
    "- It's useful when the target variable has a wide range of values and you want to penalize underpredictions more than overpredictions.\n",
    "- RMSLE is calculated as the square root of the mean of the squared differences between the natural logarithm of the predicted and true values.\n",
    "\n",
    "ii. Root Mean Squared Error (RMSE)\n",
    "- RMSE measures the average magnitude of the errors between predicted and true values, with errors squared to account for both underpredictions and overpredictions.\n",
    "- It's commonly used and interpretable in the same units as the target variable.\n",
    "- RMSE is calculated as the square root of the mean of the squared differences between predicted and true values.\n",
    "\n",
    "iii. Mean Squared Error (MSE)\n",
    "- MSE is similar to RMSE but without taking the square root. It's the average of the squared differences between predicted and true values.\n",
    "- It's used to measure the average squared deviation between predicted and true values.\n",
    "\n",
    "iv. Mean Absolute Error (MAE)\n",
    "- MAE measures the average absolute differences between predicted and true values.\n",
    "- It's less sensitive to outliers compared to RMSE because it doesn't square the errors.\n",
    "- MAE is calculated as the mean of the absolute differences between predicted and true values.\n",
    "\n",
    "##### Why MAE might be greater than the others\n",
    "- MAE could be greater than RMSE, RMSLE, or MSE in certain cases, particularly when the errors between predicted and true values are spread out and not concentrated around zero. This can happen when there are outliers or when the distribution of errors is skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "- Null Hypothesis (H0): \"There is no significant relationship between store sales and promotions.\"\n",
    "\n",
    "- Alternative Hypothesis (H1): \"There is a significant relationship between store sales and churn promotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical features\n",
    "\n",
    "train_data['onpromotion_encoded'] = np.where(train_data['onpromotion'] >= 1, 1, 0)\n",
    "train_data['onpromotion_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the sales of products when they are on promotion and when they are not\n",
    "sales_on_promotion = train_data[train_data['onpromotion_encoded'] == 1]['sales']\n",
    "sales_not_on_promotion = train_data[train_data['onpromotion_encoded'] == 0]['sales']\n",
    "\n",
    "# Calculate the sample sizes\n",
    "n1 = len(sales_on_promotion)\n",
    "n2 = len(sales_not_on_promotion)\n",
    "\n",
    "# Calculate the degrees of freedom\n",
    "degrees_of_freedom = n1 + n2 - 2  # Assuming equal variances, subtract 2 for two samples\n",
    "\n",
    "# Set the significance level\n",
    "alpha = 0.05\n",
    "\n",
    "# Calculate the critical t-values for a two-tailed test\n",
    "critical_t_value = t.ppf(1 - alpha / 2, degrees_of_freedom)\n",
    "\n",
    "# Perform an independent two-sample t-test\n",
    "t_stat, p_value = ttest_ind(sales_on_promotion, sales_not_on_promotion)\n",
    "\n",
    "# Print the t-statistic, p-value, degrees of freedom, and critical t-values\n",
    "print(\"t-statistic:\", t_stat)\n",
    "print(\"p-value:\", p_value)\n",
    "print(\"Degrees of Freedom:\", degrees_of_freedom)\n",
    "print(f\"Critical t-value (α = {alpha/2}):\", -critical_t_value)  # Left tail\n",
    "print(f\"Critical t-value (α = {alpha/2}):\", critical_t_value)   # Right tail\n",
    "\n",
    "# Check if the absolute t-statistic falls within the critical value range\n",
    "if t_stat >= -critical_t_value and t_stat <= critical_t_value:\n",
    "    print(\"\\nThe absolute t-statistic falls within the critical value range.\")\n",
    "else:\n",
    "    print(\"\\nThe absolute t-statistic does not fall within the critical value range.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging The Train Dataset with the Stores, Transactions, Holiday Events and Oil Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the common columns ('store_nbr' and 'date') in the datasets using the inner merge() function\n",
    "# Merge train_data with stores_df based on 'store_nbr' column\n",
    "merged_df1 = train_data.merge(stores_data, on='store_nbr', how='inner')\n",
    "\n",
    "# Merge merged_df1 with transactions_df based on 'date' and 'store_nbr' columns\n",
    "merged_df2 = merged_df1.merge(transactions_data, on=['date', 'store_nbr'], how='inner')\n",
    "\n",
    "# Merge merged_df2 with holidays_events_df based on 'date' column\n",
    "merged_df3 = merged_df2.merge(holidays_events_data, on='date', how='inner')\n",
    "\n",
    "# Merge merged_df3 with oil_df based on 'date' column\n",
    "merged_data = merged_df3.merge(oil_data, on='date', how='inner')\n",
    "\n",
    "# View the first five rows of the merged dataset\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use of an inner merge in this time series forecasting project for corporation Favorita helps us ensure data consistency, avoid missing values, and focus on the relevant data for accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the merged dataset\n",
    "merged_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the column information of the merged dataset\n",
    "merged_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values of the two unknown columns\n",
    "print(\"Unique values of 'type_x':\")\n",
    "print(merged_data['type_x'].unique())\n",
    "print()\n",
    "print(\"Unique values of 'type_y':\")\n",
    "print(merged_data['type_y'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns with the approapriate names\n",
    "merged_train_data = merged_data.rename(columns={\"type_x\": \"store_type\", \"type_y\": \"holiday_type\"})\n",
    "merged_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary statistics and transpose the rows and columns of the resultant DataFrame.\n",
    "# Transposing flips the DataFrame (the rows become columns and the columns become rows) for better readability\n",
    "merged_train_data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values for the merged Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the merged datasets\n",
    "missing_values = merged_train_data.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Duplicate Values in the Merged Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate values in the merged dataset\n",
    "duplicate_rows_merged = merged_train_data.duplicated()\n",
    "duplicate_rows_merged.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the merged dataset in a new CSV file to be used in PowerBI Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.to_csv('Visualization_Data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate, Multivariate and Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Distribution of the 'sales' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Histogram\n",
    "plt.hist(merged_train_data['sales'], bins=20)\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sales')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged_train_data['sales'])\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Boxplot of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. distribution of the 'transactions' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged_train_data['transactions'], bins=20)\n",
    "plt.xlabel('Transactions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Transactions')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Distribution of the 'Daily Oil Price' variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged_train_data['dcoilwtico'], bins=20)\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Oil Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii. Bivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Trend of sales over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by date and calculate the total sales\n",
    "daily_sales = merged_train_data.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Create a time series plot with slider\n",
    "fig = px.line(daily_sales, x='date', y='sales')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(title='Trend of Sales Over Time', title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Trend of Daily Crude oil Prices Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 'dcoilwtico' column to confirm if the trend is consistent.\n",
    "fig = px.line(oil_data, x='date', y='dcoilwtico')\n",
    "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii. Multivariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation matrix of numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical variables for correlation analysis\n",
    "numerical_vars = ['sales', 'transactions', 'dcoilwtico']\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr_matrix = merged_train_data[numerical_vars].corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample data to monthly frequency and compute the mean of sales for each month\n",
    "cols = ['date', 'sales']\n",
    "\n",
    "merged_monthly_mean = merged_train_data[cols].set_index('date').resample('M').mean()\n",
    "print(merged_monthly_mean.shape)\n",
    "merged_monthly_mean.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationarity implies that the statistical properties of the time series, such as mean and variance, remain constant over time. In this case, the ADF test was conducted on the 'sales' data from the 'merged_df' dataset. To perform the stationarity test, we will use the Augmented Dickey-Fuller (ADF) test commonly used to check for stationarity in a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Null hypothesis (H0): The sales data is non-stationary.\n",
    "- Alternative hypothesis (H1): The sales data is stationary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Test of the 'sales' column in the merged_df using Adfuller\n",
    "sales_data = merged_train_data['sales']\n",
    "\n",
    "# Perform ADF test\n",
    "result = adfuller(sales_data)\n",
    "\n",
    "# Extract the test statistics and p-value from the result\n",
    "test_statistic = result[0]\n",
    "p_value = result[1]\n",
    "critical_values = result[4]\n",
    "\n",
    "# Print the test statistics and critical values\n",
    "print(f\"ADF Test Statistics: {test_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(\"Critical Values:\")\n",
    "for key, value in critical_values.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Check the p-value against a significance level (e.g., 0.05)\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis: The sales data is stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The sales data is non-stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ADF test, the test statistics (-43.83) is significantly lower than the critical values at all confidence levels (1%, 5%, and 10%). Additionally, the p-value is 0.0, which is lower than the significance level of 0.05.\n",
    "\n",
    "Since the p-value is less than 0.05, we reject the null hypothesis, indicating that the sales data is stationary. The test results suggest that the 'sales' column exhibits stationarity, which means the data has a constant mean and variance over time. This property is essential for time-series analysis and modeling, as it helps to ensure reliable forecasting and prediction of future sales trends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KPSS Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "stats, p, lags, critical_values=kpss(merged_train_data['sales'], 'ct')\n",
    "\n",
    "print(f'Test_statistics: {stats}')\n",
    "print(f'p-value: {p}')\n",
    "print(f'Critical values: {critical_values}')\n",
    "\n",
    "if p < 0.05 :\n",
    "    print('Series is not stationary')\n",
    "else :\n",
    "    print('Series is stationary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first-order differencing on the target variable\n",
    "differenced_data = merged_monthly_mean - merged_monthly_mean.shift(1)\n",
    "\n",
    "# Plot the original and differenced time series\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(211)\n",
    "plt.plot(merged_monthly_mean, label='Monthly mean')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Monthly mean')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(differenced_data, label='Differenced')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Differenced Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Date Components (Day, Month, Year and Day of The Week)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Date Components\n",
    "merged_train_data['date'] = pd.to_datetime(merged_train_data['date'])\n",
    "merged_train_data['year'] = merged_train_data['date'].dt.year\n",
    "merged_train_data['month'] = merged_train_data['date'].dt.month\n",
    "merged_train_data['day'] = merged_train_data['date'].dt.day\n",
    "merged_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Unneccessary Columns in The Merged and Test Datasets as it is not needed for our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['date','id', 'locale', 'locale_name','year_x','year_y','description', 'store_type', 'transferred', 'state']\n",
    "merged_train_data = merged_train_data.drop(columns=columns_to_drop)\n",
    "\n",
    "merged_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product Categorization Based on Families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_families = merged_train_data['family'].unique()\n",
    "unique_families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the category lists for each product category\n",
    "food_families = ['BEVERAGES', 'BREAD/BAKERY', 'FROZEN FOODS', 'MEATS', 'PREPARED FOODS', 'DELI','PRODUCE', 'DAIRY','POULTRY','EGGS','SEAFOOD']\n",
    "home_families = ['HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES']\n",
    "clothing_families = ['LINGERIE', 'LADYSWARE']\n",
    "grocery_families = ['GROCERY I', 'GROCERY II']\n",
    "stationery_families = ['BOOKS', 'MAGAZINES','SCHOOL AND OFFICE SUPPLIES']\n",
    "cleaning_families = ['HOME CARE', 'BABY CARE','PERSONAL CARE']\n",
    "hardware_families = ['PLAYERS AND ELECTRONICS','HARDWARE']\n",
    "\n",
    "# Categorize the 'family' column based on the product categories\n",
    "merged_train_data['family'] = np.where(merged_train_data['family'].isin(food_families), 'FOODS', merged_train_data['family'])\n",
    "merged_train_data['family'] = np.where(merged_train_data['family'].isin(home_families), 'HOME', merged_train_data['family'])\n",
    "merged_train_data['family'] = np.where(merged_train_data['family'].isin(clothing_families), 'CLOTHING', merged_train_data['family'])\n",
    "merged_train_data['family'] = np.where(merged_train_data['family'].isin(grocery_families), 'GROCERY', merged_train_data['family'])\n",
    "merged_train_data['family'] = np.where(merged_train_data['family'].isin(stationery_families), 'STATIONERY', merged_train_data['family'])\n",
    "merged_train_data['family'] = np.where(merged_train_data['family'].isin(cleaning_families), 'CLEANING', merged_train_data['family'])\n",
    "merged_train_data['family'] = np.where(merged_train_data['family'].isin(hardware_families), 'HARDWARE', merged_train_data['family'])\n",
    "\n",
    "# Print the updated DataFrame\n",
    "merged_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling Numeric Variables (Min-Max Scaling)\n",
    "# create an instance of StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# select numerical columns\n",
    "num_cols = ['sales', 'transactions', 'dcoilwtico']\n",
    "\n",
    "# fit and transform the numerical columns\n",
    "merged_train_data[num_cols] = scaler.fit_transform(merged_train_data[num_cols])\n",
    "\n",
    "# Display the updated dataframe\n",
    "merged_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding The Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the categorical columns to encode\n",
    "categorical_columns = [\"family\", \"city\", \"holiday_type\"]\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "one_hot_encoded_data = encoder.fit_transform(merged_train_data[categorical_columns])\n",
    "\n",
    "# Create column names for the one-hot encoded data\n",
    "column_names = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Convert the one-hot encoded data to a DataFrame\n",
    "merged_df_encoded = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
    "\n",
    "# Concatenate the original dataframe with the one-hot encoded data\n",
    "merged_df_encoded = pd.concat([merged_train_data, merged_df_encoded], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "merged_df_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
    "\n",
    "# Print the head of the encoded DataFrame\n",
    "merged_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Date Components\n",
    "test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "test_data['year'] = test_data['date'].dt.year\n",
    "test_data['month'] = test_data['date'].dt.month\n",
    "test_data['day'] = test_data['date'].dt.day\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Unnecessary Columns\n",
    "columns_to_drop = ['date', 'id']\n",
    "test_df = test_data.drop(columns=columns_to_drop)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Categorization Based on Families\n",
    "food_families = ['BEVERAGES', 'BREAD/BAKERY', 'FROZEN FOODS', 'MEATS', 'PREPARED FOODS', 'DELI', 'PRODUCE', 'DAIRY', 'POULTRY', 'EGGS', 'SEAFOOD']\n",
    "home_families = ['HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES']\n",
    "clothing_families = ['LINGERIE', 'LADYSWARE']\n",
    "grocery_families = ['GROCERY I', 'GROCERY II']\n",
    "stationery_families = ['BOOKS', 'MAGAZINES', 'SCHOOL AND OFFICE SUPPLIES']\n",
    "cleaning_families = ['HOME CARE', 'BABY CARE', 'PERSONAL CARE']\n",
    "hardware_families = ['PLAYERS AND ELECTRONICS', 'HARDWARE']\n",
    "\n",
    "test_df['family'] = np.where(test_df['family'].isin(food_families), 'FOODS', test_df['family'])\n",
    "test_df['family'] = np.where(test_df['family'].isin(home_families), 'HOME', test_df['family'])\n",
    "test_df['family'] = np.where(test_df['family'].isin(clothing_families), 'CLOTHING', test_df['family'])\n",
    "test_df['family'] = np.where(test_df['family'].isin(grocery_families), 'GROCERY', test_df['family'])\n",
    "test_df['family'] = np.where(test_df['family'].isin(stationery_families), 'STATIONERY', test_df['family'])\n",
    "test_df['family'] = np.where(test_df['family'].isin(cleaning_families), 'CLEANING', test_df['family'])\n",
    "test_df['family'] = np.where(test_df['family'].isin(hardware_families), 'HARDWARE', test_df['family'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding The Categorical Variables\n",
    "# List of categorical columns to encode\n",
    "categorical_columns = [\"family\"]\n",
    "\n",
    "# Create an instance of the OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Perform one-hot encoding on the 'test_df' data for the specified categorical columns\n",
    "one_hot_encoded_data = encoder.fit_transform(test_df[categorical_columns])\n",
    "\n",
    "# Get the column names for the one-hot encoded data\n",
    "column_names = encoder.get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Create a DataFrame with the one-hot encoded data and corresponding column names\n",
    "test_df_encoded = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
    "\n",
    "# Concatenate the original 'test_df' with the one-hot encoded data\n",
    "test_df_encoded = pd.concat([test_df, test_df_encoded], axis=1)\n",
    "\n",
    "# Drop the original categorical columns since they have been encoded\n",
    "test_df_encoded.drop(categorical_columns, axis=1, inplace=True)\n",
    "\n",
    "# Display the updated 'test_df_encoded' DataFrame\n",
    "test_df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = merged_df_encoded.loc[merged_df_encoded['year'].isin([2013, 2014, 2015, 2016])]\n",
    "eval_set = merged_df_encoded.loc[merged_df_encoded['year'] == 2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to the shape after data splitting\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking to the shape after data splitting\n",
    "eval_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable and features for training and testing\n",
    "X_train = train_set.drop('sales', axis=1)\n",
    "y_train = train_set['sales'] \n",
    "\n",
    "\n",
    "X_eval = eval_set.drop('sales', axis=1)  \n",
    "y_eval = eval_set['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the results dataframe\n",
    "results_df = pd.DataFrame(columns=['Model', 'RMSLE', 'RMSE', 'MSE', 'MAE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original 'Month' and 'Day' columns from X_train\n",
    "X_train.drop(columns=['Month', 'Day'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Day' and 'Month' columns from X_eval\n",
    "X_eval.drop(columns=['Day', 'Month'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store results\n",
    "results_df = pd.DataFrame()\n",
    "# Linear Regression Model\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_predictions = lr_model.predict(X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "lr_mse = mean_squared_error(y_eval, lr_predictions)\n",
    "lr_mae = mean_absolute_error(y_eval, lr_predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_eval and lr_predictions\n",
    "y_eval_abs = abs(y_eval)\n",
    "lr_predictions_abs = abs(lr_predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "lr_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, lr_predictions_abs))\n",
    "\n",
    "# Create a DataFrame to store results for Linear Regression\n",
    "results_lr = pd.DataFrame({'Model': ['Linear Regression'],\n",
    "                            'RMSLE': [lr_rmsle],\n",
    "                            'RMSE': [np.sqrt(lr_mse)],\n",
    "                            'MSE': [lr_mse],\n",
    "                            'MAE': [lr_mae]}).round(2)\n",
    "\n",
    "# Print the results_lr dataframe\n",
    "results_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Regression Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_predictions = rf_model.predict(X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "rf_mse = mean_squared_error(y_eval, rf_predictions)\n",
    "rf_mae = mean_absolute_error(y_eval, rf_predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_eval and rf_predictions\n",
    "y_eval_abs = abs(y_eval)\n",
    "rf_predictions_abs = abs(rf_predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "rf_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, rf_predictions_abs))\n",
    "\n",
    "# Create a DataFrame to store results for Random Forest\n",
    "results_rf = pd.DataFrame({'Model': ['Random Forest'],\n",
    "                            'RMSLE': [rf_rmsle],\n",
    "                            'RMSE': [np.sqrt(rf_mse)],\n",
    "                            'MSE': [rf_mse],\n",
    "                            'MAE': [rf_mae]}).round(2)\n",
    "\n",
    "# Print the results_rf dataframe\n",
    "results_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3. Gradient Boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting Regression Model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "gb_predictions = gb_model.predict(X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "gb_mse = mean_squared_error(y_eval, gb_predictions)\n",
    "gb_mae = mean_absolute_error(y_eval, gb_predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_eval and gb_predictions\n",
    "y_eval_abs = abs(y_eval)\n",
    "gb_predictions_abs = abs(gb_predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "gb_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, gb_predictions_abs))\n",
    "\n",
    "# Create a DataFrame to store results for Gradient Boosting\n",
    "results_gb = pd.DataFrame({'Model': ['Gradient Boosting'],\n",
    "                            'RMSLE': [gb_rmsle],\n",
    "                            'RMSE': [np.sqrt(gb_mse)],\n",
    "                            'MSE': [gb_mse],\n",
    "                            'MAE': [gb_mae]}).round(2)\n",
    "\n",
    "# Print the results_gb dataframe\n",
    "results_gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4. ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA Model\n",
    "# d and q are equal to zero as data is already stationary\n",
    "p = 1\n",
    "d = 0 \n",
    "q = 0  \n",
    "\n",
    "# Create an instance of the ARIMA model\n",
    "arima_model = ARIMA(y_train, order=(p, d, q))\n",
    "\n",
    "# Fit the model to the training data\n",
    "arima_model_fit = arima_model.fit()\n",
    "\n",
    "# Make predictions on the evaluation data\n",
    "arima_predictions = arima_model_fit.predict(start=len(y_train), end=len(y_train) + len(X_eval) - 1)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "arima_mse = mean_squared_error(y_eval, arima_predictions)\n",
    "arima_rmse = np.sqrt(arima_mse)\n",
    "\n",
    "# Apply the absolute value function to y_eval to remove negative signs\n",
    "y_eval_abs = abs(y_eval)\n",
    "arima_predictions_abs = abs(arima_predictions)\n",
    "\n",
    "# Calculate the Mean Absolute Error (MAE)\n",
    "arima_mae = mean_absolute_error(y_eval, arima_predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "arima_rmsle = np.sqrt(mean_squared_log_error(y_eval_abs, arima_predictions_abs))\n",
    "\n",
    "# Create a DataFrame to store results for ARIMA\n",
    "results_arima = pd.DataFrame({'Model': ['ARIMA'],\n",
    "                            'RMSLE': [arima_rmsle],\n",
    "                            'RMSE': [np.sqrt(arima_mse)],\n",
    "                            'MSE': [arima_mse],\n",
    "                            'MAE': [arima_mae]}).round(2)\n",
    "\n",
    "# Print the results_arima dataframe\n",
    "results_arima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5. SARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the order and seasonal order parameters\n",
    "# Seasonal autoregressive order\n",
    "P = 0  \n",
    "# Seasonal differencing order\n",
    "D = 0  \n",
    "# Seasonal moving average order\n",
    "Q = 0  \n",
    "# Number of time steps in each season (chosen based on the number of months each year)\n",
    "s = 12  \n",
    "\n",
    "# Convert endogenous variable (y_train) to NumPy array\n",
    "y_train_np = np.asarray(y_train)\n",
    "\n",
    "# Convert exogenous variables (X_train) to NumPy array and handle non-finite values\n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "X_train_df.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "\n",
    "# Ensure that the exogenous variable is correctly converted to a NumPy array with a numeric data type\n",
    "X_train_np = X_train_df.apply(pd.to_numeric, errors='coerce').fillna(0).values \n",
    "\n",
    "# Ensure that the entire array is of a numeric data type\n",
    "X_train_np = np.asarray(X_train_np, dtype=np.float64)\n",
    "# Create an instance of the SARIMA model\n",
    "sarima_model = SARIMAX(endog=y_train, exog=X_train_df, order=(p, d, q), seasonal_order=(P, D, Q, s))\n",
    "\n",
    "# Fit the model to the training data\n",
    "sarima_fit = sarima_model.fit()\n",
    "\n",
    "# Make predictions on the evaluation data\n",
    "sarima_predictions = sarima_fit.forecast(steps=len(y_eval), exog=X_eval)\n",
    "\n",
    "# Calculate metrics\n",
    "sarima_mse = mean_squared_error(y_eval, sarima_predictions)\n",
    "sarima_rmse = np.sqrt(sarima_mse)\n",
    "sarima_mae = mean_absolute_error(y_eval, sarima_predictions)\n",
    "sarima_rmsle = np.sqrt(mean_squared_error(np.log1p(y_eval), np.log1p(sarima_predictions)))\n",
    "\n",
    "# Create a DataFrame to store results for SARIMA\n",
    "results_sarima = pd.DataFrame({'Model': ['SARIMA'],\n",
    "                                'RMSLE': [sarima_rmsle],\n",
    "                                'RMSE': [sarima_rmse],\n",
    "                                'MSE': [sarima_mse],\n",
    "                                'MAE': [sarima_mae]}).round(2)\n",
    "\n",
    "# Print the results_sarima dataframe\n",
    "results_sarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights:\n",
    "\n",
    "- Random Forest and Gradient Boosting models have similar performance, with the lowest RMSLE and RMSE among all models.\n",
    "- Linear Regression performs slightly worse than the tree-based models, with a higher RMSLE, RMSE, MSE, and MAE.\n",
    "- SARIMA and ARIMA models have higher RMSLE and RMSE compared to the other models, indicating poorer performance in capturing the underlying patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for tuning the random forest model\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_split': [2, 4, 6],\n",
    "    'min_samples_leaf': [1, 2, 3],\n",
    "    'max_features': ['sqrt', 'log2', 0.5]\n",
    "}\n",
    "\n",
    "# Create Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search_rf = RandomizedSearchCV(rf_model, param_distributions=param_grid_rf,\n",
    "                                      n_iter=10, scoring='neg_mean_squared_error', cv=5,\n",
    "                                      n_jobs=-1, random_state=42)\n",
    "\n",
    "# Fit RandomizedSearchCV to the data\n",
    "random_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and its hyperparameters\n",
    "best_rf_model = random_search_rf.best_estimator_\n",
    "best_rf_params = random_search_rf.best_params_\n",
    "\n",
    "# Make predictions using the best model\n",
    "best_rf_predictions = best_rf_model.predict(X_eval)\n",
    "\n",
    "# Calculate metrics for the best model\n",
    "best_rf_mse = mean_squared_error(y_eval, best_rf_predictions)\n",
    "best_rf_rmse = np.sqrt(best_rf_mse)\n",
    "best_rf_mae = mean_absolute_error(y_eval, best_rf_predictions)\n",
    "\n",
    "# Apply absolute value to both predicted and target values\n",
    "abs_best_rf_predictions = np.abs(best_rf_predictions)\n",
    "abs_y_eval = np.abs(y_eval)\n",
    "\n",
    "# Calculate RMSLE using the absolute values\n",
    "best_rf_rmsle = np.sqrt(mean_squared_log_error(abs_y_eval, abs_best_rf_predictions))\n",
    "\n",
    "# Create a DataFrame to store results for the best Random Forest model\n",
    "best_results_rf = pd.DataFrame({'Model': ['Best Random Forest'],\n",
    "                                'RMSLE': [best_rf_rmsle],\n",
    "                                'RMSE': [best_rf_rmse],\n",
    "                                'MSE': [best_rf_mse],\n",
    "                                'MAE': [best_rf_mae]}).round(2)\n",
    "\n",
    "# Print the best_results_rf dataframe\n",
    "best_results_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "print(\"Best Parameters for Random Forest Model:\")\n",
    "print(best_rf_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Best RF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the key components\n",
    "key_components = {\n",
    "    'model': best_rf_model, \n",
    "    'best_params': best_rf_params,\n",
    "    'best_score': best_rf_rmsle\n",
    "}\n",
    "\n",
    "# Save the key components in a file using pickle\n",
    "with open('best_rf_model_components.pkl', 'wb') as file:\n",
    "    pickle.dump(key_components, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
